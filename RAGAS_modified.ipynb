{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5e681ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3699: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "e:\\RAGAS\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- LangChain and LLM Imports ---\n",
    "from langchain_openai import ChatOpenAI \n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# --- Document Loading and Vector Store ---\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings \n",
    "\n",
    "# --- Prompting and Document Utilities ---\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "# --- Core and Output Parsers ---\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "# --- LangGraph for Workflow Graphs ---\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "from time import monotonic\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "# --- Datasets and Typing ---\n",
    "from datasets import Dataset\n",
    "from typing_extensions import TypedDict\n",
    "from IPython.display import display, Image\n",
    "from typing import List, TypedDict\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_correctness,\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    answer_similarity\n",
    ")\n",
    "from pathlib import Path\n",
    "\n",
    "import langgraph\n",
    "\n",
    "\n",
    "\n",
    "# --- Load environment variables (e.g., API keys) ---\n",
    "load_dotenv(dotenv_path=Path().resolve() / \".env\", override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a03651e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the OpenAI API key from environment variable (for use by OpenAI LLMs)\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# Retrieve the Groq API key from environment variable (for use by Groq LLMs)\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "440691f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the Harry Potter PDF file.\n",
    "# This variable will be used throughout the notebook for loading and processing the book.\n",
    "hp_pdf_path =r\"C:\\Users\\NAFEES J\\Downloads\\Harry Potter - Book 1 - The Sorcerers Stone.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6bea1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import split_into_chapters, replace_t_with_space\n",
    "# --- Split the PDF into chapters and preprocess the text ---\n",
    "\n",
    "# 1. Split the PDF into chapters using the provided helper function.\n",
    "#    This function takes the path to the PDF and returns a list of Document objects, each representing a chapter.\n",
    "chapters = split_into_chapters(hp_pdf_path)\n",
    "\n",
    "# 2. Clean up the text in each chapter by replacing unwanted characters (e.g., '\\t') with spaces.\n",
    "#    This ensures the text is consistent and easier to process downstream.\n",
    "chapters = replace_t_with_space(chapters)\n",
    "\n",
    "# 3. Print the number of chapters extracted to verify the result.\n",
    "print(len(chapters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74e434d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "ðŸ“„ document_cleaned length: 221\n",
      "abc\n",
      "Found 2 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 13 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 1 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 12 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 15 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 1 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 13 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 12 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 1 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 1 quotes on one doc\n",
      "Found 14 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 15 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 12 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 14 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 13 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "âœ… Quotes Extracted: 1299\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import helper_functions\n",
    "importlib.reload(helper_functions)\n",
    "\n",
    "# --- Load and Preprocess the PDF, then Extract Quotes ---\n",
    "\n",
    "# 1. Load the PDF\n",
    "loader = PyPDFLoader(hp_pdf_path)\n",
    "document = loader.load()\n",
    "\n",
    "# 2. Clean the document (remove tabs)\n",
    "document_cleaned = helper_functions.replace_t_with_space(document)\n",
    "print(\"ðŸ“„ document_cleaned length:\", len(document_cleaned))\n",
    "\n",
    "# 3. Extract quotes as Documents\n",
    "book_quotes_list = helper_functions.extract_book_quotes_as_documents(document_cleaned)\n",
    "\n",
    "print(\"âœ… Quotes Extracted:\", len(book_quotes_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cf0e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Summarization Prompt Template for LLM-based Summarization ---\n",
    "\n",
    "# Define the template string for summarization.\n",
    "# This template instructs the language model to write an extensive summary of the provided text.\n",
    "summarization_prompt_template = \"\"\"Write an extensive summary of the following:\n",
    "\n",
    "{text}\n",
    "\n",
    "SUMMARY:\"\"\"\n",
    "\n",
    "# Create a PromptTemplate object using the template string.\n",
    "# The input variable \"text\" will be replaced with the content to summarize.\n",
    "summarization_prompt = PromptTemplate(\n",
    "    template=summarization_prompt_template,\n",
    "    input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e40db0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "importlib.reload(helper_functions)\n",
    "\n",
    "\n",
    "# Gemini doesn't have an official tokenizer, so we use a rough heuristic\n",
    "def num_tokens_from_string(string: str, model_name: str) -> int:\n",
    "    return len(string) // 4  # Rough estimate: 4 characters per token\n",
    "\n",
    "def create_chapter_summary(chapter):\n",
    "    \"\"\"\n",
    "    Creates a summary of a chapter using a large language model (LLM).\n",
    "\n",
    "    Args:\n",
    "        chapter: A Document object representing the chapter to summarize.\n",
    "\n",
    "    Returns:\n",
    "        A Document object containing the summary of the chapter.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the text content from the chapter\n",
    "    chapter_txt = chapter.page_content\n",
    "\n",
    "    # Specify the LLM model and configuration\n",
    "    model_name = \"gemini-2.5-pro\"\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=model_name,\n",
    "        temperature=0,\n",
    "        convert_system_message_to_human=True\n",
    "    )\n",
    "    gpt_35_turbo_max_tokens = 16000  # Keep your original logic/variable name\n",
    "    verbose = False  # Set to True for more detailed output\n",
    "\n",
    "    # Calculate the number of tokens in the chapter text\n",
    "    num_tokens = num_tokens_from_string(chapter_txt, model_name)\n",
    "\n",
    "    # Choose the summarization chain type based on token count\n",
    "    if num_tokens < gpt_35_turbo_max_tokens:\n",
    "        # For shorter chapters, use the \"stuff\" chain type\n",
    "        chain = load_summarize_chain(\n",
    "            llm,\n",
    "            chain_type=\"stuff\",\n",
    "            prompt=summarization_prompt,\n",
    "            verbose=verbose\n",
    "        )\n",
    "    else:\n",
    "        # For longer chapters, use the \"map_reduce\" chain type\n",
    "        chain = load_summarize_chain(\n",
    "            llm,\n",
    "            chain_type=\"map_reduce\",\n",
    "            map_prompt=summarization_prompt,\n",
    "            combine_prompt=summarization_prompt,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "    # Start timer to measure summarizatime\n",
    "    start_time = monotonic()\n",
    "\n",
    "    # Create a Document object for the chapter\n",
    "    doc_chapter = Document(page_content=chapter_txt)\n",
    "\n",
    "    # Generate the summary using the selected chain\n",
    "    summary_result = chain.invoke([doc_chapter])\n",
    "\n",
    "    # Print chain type and execution time for reference\n",
    "    print(f\"Chain type: {chain.__class__.__name__}\")\n",
    "    print(f\"Run time: {monotonic() - start_time}\")\n",
    "\n",
    "    # Clean up the summary text (remove double newlines, etc.)\n",
    "    summary_text = helper_functions.replace_double_lines_with_one_line(summary_result[\"output_text\"])\n",
    "\n",
    "    # Create a Document object for the summary, preserving chapter metadata\n",
    "    doc_summary = Document(page_content=summary_text, metadata=chapter.metadata)\n",
    "\n",
    "    return doc_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8de58e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chapter 14 of 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain type: StuffDocumentsChain\n",
      "Run time: 38.26162130001467\n",
      "Checkpoint saved at chapter 14\n",
      "Processing chapter 15 of 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain type: StuffDocumentsChain\n",
      "Run time: 42.91590920003364\n",
      "Checkpoint saved at chapter 15\n",
      "Processing chapter 16 of 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain type: StuffDocumentsChain\n",
      "Run time: 46.75584999995772\n",
      "Checkpoint saved at chapter 16\n",
      "Processing chapter 17 of 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain type: StuffDocumentsChain\n",
      "Run time: 45.50914509996073\n",
      "Checkpoint saved at chapter 17\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Where to save the checkpoint\n",
    "checkpoint_path = Path(\"chapter_summaries_checkpoint.pkl\")\n",
    "\n",
    "# Load checkpoint if exists\n",
    "if checkpoint_path.exists():\n",
    "    with open(checkpoint_path, \"rb\") as f:\n",
    "        chapter_summaries = pickle.load(f)\n",
    "    start_index = len(chapter_summaries)\n",
    "else:\n",
    "    chapter_summaries = []\n",
    "    start_index = 0\n",
    "\n",
    "# Set how many chapters to process per run\n",
    "max_chapters_per_run = 10\n",
    "total_chapters = 17  # or len(chapters), but you specified 1â€“17\n",
    "end_index = min(start_index + max_chapters_per_run, total_chapters)\n",
    "\n",
    "# Process chapters from start_index to end_index - 1\n",
    "for i in range(start_index, end_index):\n",
    "    chapter = chapters[i]\n",
    "    print(f\"Processing chapter {i+1} of {total_chapters}\")\n",
    "    \n",
    "    try:\n",
    "        summary = create_chapter_summary(chapter)\n",
    "        chapter_summaries.append(summary)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chapter {i+1}: {e}\")\n",
    "        break  # or continue to skip errors\n",
    "\n",
    "    # Save checkpoint after each chapter for safety\n",
    "    with open(checkpoint_path, \"wb\") as f:\n",
    "        pickle.dump(chapter_summaries, f)\n",
    "    print(f\"Checkpoint saved at chapter {i+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80a756e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def encode_book(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Encodes a PDF book into a FAISS vector store using HuggingFace embeddings.\n",
    "\n",
    "    Args:\n",
    "        path (str): The path to the PDF file.\n",
    "        chunk_size (int): The desired size of each text chunk.\n",
    "        chunk_overlap (int): The amount of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        FAISS: A FAISS vector store containing the encoded book content.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Load the PDF document using PyPDFLoader\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # 2. Split the document into chunks for embedding\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    # 3. Clean up the text chunks (replace unwanted characters)\n",
    "    cleaned_texts = replace_t_with_space(texts)\n",
    "\n",
    "    # 4. Create HuggingFace embeddings and encode the cleaned text chunks into a FAISS vector store\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
    "\n",
    "    # 5. Return the vector store\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69f1deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_chapter_summaries(chapter_summaries):\n",
    "    \"\"\"\n",
    "    Encodes a list of chapter summaries into a FAISS vector store using HuggingFace embeddings.\n",
    "\n",
    "    Args:\n",
    "        chapter_summaries (list): A list of Document objects representing the chapter summaries.\n",
    "\n",
    "    Returns:\n",
    "        FAISS: A FAISS vector store containing the encoded chapter summaries.\n",
    "    \"\"\"\n",
    "    # Create HuggingFace embeddings instance\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Encode the chapter summaries into a FAISS vector store\n",
    "    chapter_summaries_vectorstore = FAISS.from_documents(chapter_summaries, embeddings)\n",
    "    \n",
    "    # Return the vector store\n",
    "    return chapter_summaries_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "516ef92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_quotes(book_quotes_list):\n",
    "    \"\"\"\n",
    "    Encodes a list of book quotes into a FAISS vector store using HuggingFace embeddings.\n",
    "\n",
    "    Args:\n",
    "        book_quotes_list (list): A list of Document objects, each representing a quote from the book.\n",
    "\n",
    "    Returns:\n",
    "        FAISS: A FAISS vector store containing the encoded book quotes.\n",
    "    \"\"\"\n",
    "    # Create HuggingFace embeddings instance\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Encode the book quotes into a FAISS vector store\n",
    "    quotes_vectorstore = FAISS.from_documents(book_quotes_list, embeddings)\n",
    "    \n",
    "    # Return the vector store\n",
    "    return quotes_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43bea260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NAFEES J\\AppData\\Local\\Temp\\ipykernel_22152\\3118181819.py:11: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# --- Create or Load Vector Stores for Book Chunks, Chapter Summaries, and Book Quotes ---\n",
    "\n",
    "# Check if the vector stores already exist on disk\n",
    "if (\n",
    "    os.path.exists(\"chunks_vector_store\") and\n",
    "    os.path.exists(\"chapter_summaries_vector_store\") and\n",
    "    os.path.exists(\"book_quotes_vectorstore\")\n",
    "):\n",
    "    # If vector stores exist, load them using HuggingFace embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    chunks_vector_store = FAISS.load_local(\n",
    "        \"chunks_vector_store\", embeddings, allow_dangerous_deserialization=True\n",
    "    )\n",
    "    chapter_summaries_vector_store = FAISS.load_local(\n",
    "        \"chapter_summaries_vector_store\", embeddings, allow_dangerous_deserialization=True\n",
    "    )\n",
    "    book_quotes_vectorstore = FAISS.load_local(\n",
    "        \"book_quotes_vectorstore\", embeddings, allow_dangerous_deserialization=True\n",
    "    )\n",
    "else:\n",
    "    print(\"not\")\n",
    "    # If vector stores do not exist, encode and save them\n",
    "\n",
    "    # 1. Encode the book into a vector store of chunks\n",
    "    chunks_vector_store = encode_book(hp_pdf_path, chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "    # 2. Encode the chapter summaries into a vector store\n",
    "    chapter_summaries_vector_store = encode_chapter_summaries(chapter_summaries)\n",
    "\n",
    "    # 3. Encode the book quotes into a vector store\n",
    "    book_quotes_vectorstore = encode_quotes(book_quotes_list)\n",
    "\n",
    "    # 4. Save the vector stores to disk for future use\n",
    "    chunks_vector_store.save_local(\"chunks_vector_store\")\n",
    "    chapter_summaries_vector_store.save_local(\"chapter_summaries_vector_store\")\n",
    "    book_quotes_vectorstore.save_local(\"book_quotes_vectorstore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a4cf5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create Query Retrievers from Vector Stores ---\n",
    "\n",
    "# The following retrievers are used to fetch relevant documents from the vector stores\n",
    "# based on a query. The number of results returned can be controlled via the 'k' parameter.\n",
    "\n",
    "# Retriever for book chunks (returns the top 1 most relevant chunk)\n",
    "chunks_query_retriever = chunks_vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# Retriever for chapter summaries (returns the top 1 most relevant summary)\n",
    "chapter_summaries_query_retriever = chapter_summaries_vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# Retriever for book quotes (returns the top 10 most relevant quotes)\n",
    "book_quotes_query_retriever = book_quotes_vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06dd66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import helper_functions\n",
    "importlib.reload(helper_functions)\n",
    "def retrieve_context_per_question(state):\n",
    "    \"\"\"\n",
    "    Retrieves relevant context for a given question by aggregating content from:\n",
    "    - Book chunks\n",
    "    - Chapter summaries\n",
    "    - Book quotes\n",
    "\n",
    "    Args:\n",
    "        state (dict): A dictionary containing the question to answer, with key \"question\".\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with keys:\n",
    "            - \"context\": Aggregated context string from all sources.\n",
    "            - \"question\": The original question.\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieve relevant book chunks\n",
    "    print(\"Retrieving relevant chunks...\")\n",
    "    docs = chunks_query_retriever.get_relevant_documents(question)\n",
    "    context = \" \".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Retrieve relevant chapter summaries\n",
    "    print(\"Retrieving relevant chapter summaries...\")\n",
    "    docs_summaries = chapter_summaries_query_retriever.get_relevant_documents(question)\n",
    "    context_summaries = \" \".join(\n",
    "        f\"{doc.page_content} (Chapter {doc.metadata['chapter']})\" for doc in docs_summaries\n",
    "    )\n",
    "\n",
    "    # Retrieve relevant book quotes\n",
    "    print(\"Retrieving relevant book quotes...\")\n",
    "    docs_book_quotes = book_quotes_query_retriever.get_relevant_documents(question)\n",
    "    book_quotes = \" \".join(doc.page_content for doc in docs_book_quotes)\n",
    "    # Aggregate all contexts and escape problematic characters\n",
    "    all_contexts = context + context_summaries + book_quotes\n",
    "    all_contexts =helper_functions.escape_quotes(all_contexts)\n",
    "\n",
    "    return {\"context\": all_contexts, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "098c34d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "importlib.reload(helper_functions)\n",
    "# --- Prompt template ---\n",
    "keep_only_relevant_content_prompt_template = \"\"\"\n",
    "You receive a query: {query} and retrieved documents: {retrieved_documents} from a vector store.\n",
    "You need to filter out all the non relevant information that doesn't supply important information regarding the {query}.\n",
    "Your goal is just to filter out the non relevant information.\n",
    "You can remove parts of sentences that are not relevant to the query or remove whole sentences that are not relevant to the query.\n",
    "DO NOT ADD ANY NEW INFORMATION THAT IS NOT IN THE RETRIEVED DOCUMENTS.\n",
    "Output the filtered relevant content.\n",
    "\"\"\"\n",
    "\n",
    "# --- Output schema ---\n",
    "class KeepRelevantContent(BaseModel):\n",
    "    relevant_content: str = Field(\n",
    "        description=\"The relevant content from the retrieved documents that is relevant to the query.\"\n",
    "    )\n",
    "\n",
    "# --- Prompt + chain ---\n",
    "keep_only_relevant_content_prompt = PromptTemplate(\n",
    "    template=keep_only_relevant_content_prompt_template,\n",
    "    input_variables=[\"query\", \"retrieved_documents\"],\n",
    ")\n",
    "\n",
    "keep_only_relevant_content_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    temperature=0,\n",
    "    convert_system_message_to_human=True\n",
    ")\n",
    "\n",
    "keep_only_relevant_content_chain = (\n",
    "    keep_only_relevant_content_prompt\n",
    "    | keep_only_relevant_content_llm.with_structured_output(KeepRelevantContent)\n",
    ")\n",
    "\n",
    "# --- Filtering function ---\n",
    "def keep_only_relevant_content(state):\n",
    "    \"\"\"\n",
    "    Filters and keeps only the relevant content from the retrieved documents that is relevant to the query.\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    input_data = {\n",
    "        \"query\": question,\n",
    "        \"retrieved_documents\": context\n",
    "    }\n",
    "\n",
    "    print(\"Keeping only the relevant content...\")\n",
    "    pprint(\"--------------------\")\n",
    "    output = keep_only_relevant_content_chain.invoke(input_data)\n",
    "    relevant_content = output.relevant_content\n",
    "    relevant_content = \"\".join(relevant_content)\n",
    "\n",
    "    # Escape quotes if needed (define escape_quotes function elsewhere if used)\n",
    "    relevant_content = helper_functions.escape_quotes(relevant_content)\n",
    "\n",
    "    return {\n",
    "        \"relevant_context\": relevant_content,\n",
    "        \"context\": context,\n",
    "        \"question\": question\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c64f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM-based Function to Rewrite a Question for Better Vectorstore Retrieval ---\n",
    "\n",
    "class RewriteQuestion(BaseModel):\n",
    "    \"\"\"\n",
    "    Output schema for the rewritten question.\n",
    "    \"\"\"\n",
    "    rewritten_question: str = Field(\n",
    "        description=\"The improved question optimized for vectorstore retrieval.\"\n",
    "    )\n",
    "    explanation: str = Field(\n",
    "        description=\"The explanation of the rewritten question.\"\n",
    "    )\n",
    "\n",
    "# Create a JSON parser for the output schema\n",
    "rewrite_question_string_parser = JsonOutputParser(pydantic_object=RewriteQuestion)\n",
    "\n",
    "# Initialize the LLM for rewriting questions\n",
    "rewrite_llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"llama3-70b-8192\",\n",
    "    groq_api_key=groq_api_key,\n",
    "    max_tokens=4000\n",
    ")\n",
    "\n",
    "# Define the prompt template for question rewriting\n",
    "rewrite_prompt_template = \"\"\"\n",
    "You are a question re-writer that converts an input question to a better version optimized for vectorstore retrieval.\n",
    "Analyze the input question {question} and try to reason about the underlying semantic intent / meaning.\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "# Create the prompt object\n",
    "rewrite_prompt = PromptTemplate(\n",
    "    template=rewrite_prompt_template,\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": rewrite_question_string_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Combine prompt, LLM, and parser into a chain\n",
    "question_rewriter = rewrite_prompt | rewrite_llm | rewrite_question_string_parser\n",
    "\n",
    "def rewrite_question(state):\n",
    "    \"\"\"\n",
    "    Rewrites the given question using the LLM to optimize it for vectorstore retrieval.\n",
    "\n",
    "    Args:\n",
    "        state (dict): A dictionary containing the question to rewrite, with key \"question\".\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the rewritten question under the key \"question\".\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    print(\"Rewriting the question...\")\n",
    "    result = question_rewriter.invoke({\"question\": question})\n",
    "    new_question = result[\"rewritten_question\"]\n",
    "    return {\"question\": new_question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dad19f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    temperature=0,\n",
    "    convert_system_message_to_human=True\n",
    ")\n",
    "\n",
    "def generate_answer(state):\n",
    "    \"\"\"\n",
    "    Generates an answer to the given question using Gemini 2.5 based on filtered context.\n",
    "    \"\"\"\n",
    "\n",
    "    # âœ… Step 1: Rewrite the question using Groq LLaMA3\n",
    "    rewritten_state = rewrite_question(state)\n",
    "    question = rewritten_state[\"question\"]\n",
    "\n",
    "    # âœ… Step 2: Retrieve context based on rewritten question\n",
    "    retrieval_output = retrieve_context_per_question({\"question\": question})\n",
    "    context = retrieval_output[\"context\"]\n",
    "\n",
    "    # âœ… Step 3: Filter relevant content\n",
    "    filtered_output = keep_only_relevant_content({\n",
    "        \"question\": question,\n",
    "        \"context\": context\n",
    "    })\n",
    "    relevant_context = filtered_output[\"relevant_context\"]\n",
    "\n",
    "    # âœ… Step 4: Create prompt for Gemini\n",
    "    prompt = f\"\"\"You are a helpful assistant answering questions based on the following context from a book. \n",
    "Please check if the answer exists in the context. If yes, answer appropriately. \n",
    "If not, reply: \"Sorry, the question was out of context.\"\n",
    "\n",
    "Context:\n",
    "{relevant_context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # âœ… Step 5: Generate final response\n",
    "    response = gemini_model.invoke(prompt)\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"context\": relevant_context,\n",
    "        \"answer\": response.content.strip()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88336403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewriting the question...\n",
      "Retrieving relevant chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NAFEES J\\AppData\\Local\\Temp\\ipykernel_22152\\2536062878.py:23: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = chunks_query_retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving relevant chapter summaries...\n",
      "Retrieving relevant book quotes...\n",
      "Keeping only the relevant content...\n",
      "'--------------------'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# This is your RAG function using Gemini\n",
    "def rag_interface(question):\n",
    "    state = {\"question\": question}\n",
    "    output = generate_answer(state)\n",
    "    return output[\"answer\"]\n",
    "\n",
    "# Launch a Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=rag_interface,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Ask a question about the book...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"ðŸ“˜ Book Q&A with Gemini\",\n",
    "    description=\"Ask any question about the book. Answers are generated using Gemini and retrieved from book chunks, chapter summaries, and quotes.\"\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
