{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e681ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3699: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "e:\\RAGAS\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- LangChain and LLM Imports ---\n",
    "from langchain_openai import ChatOpenAI \n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# --- Document Loading and Vector Store ---\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings \n",
    "\n",
    "# --- Prompting and Document Utilities ---\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "# --- Core and Output Parsers ---\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "# --- LangGraph for Workflow Graphs ---\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "from time import monotonic\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "# --- Datasets and Typing ---\n",
    "from datasets import Dataset\n",
    "from typing_extensions import TypedDict\n",
    "from IPython.display import display, Image\n",
    "from typing import List, TypedDict\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_correctness,\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    answer_similarity\n",
    ")\n",
    "from pathlib import Path\n",
    "\n",
    "import langgraph\n",
    "\n",
    "\n",
    "\n",
    "# --- Load environment variables (e.g., API keys) ---\n",
    "load_dotenv(dotenv_path=Path().resolve() / \".env\", override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a03651e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the OpenAI API key from environment variable (for use by OpenAI LLMs)\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# Retrieve the Groq API key from environment variable (for use by Groq LLMs)\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "440691f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the Harry Potter PDF file.\n",
    "# This variable will be used throughout the notebook for loading and processing the book.\n",
    "hp_pdf_path =r\"C:\\Users\\NAFEES J\\Downloads\\Harry Potter - Book 1 - The Sorcerers Stone.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6bea1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import split_into_chapters, replace_t_with_space\n",
    "# --- Split the PDF into chapters and preprocess the text ---\n",
    "\n",
    "# 1. Split the PDF into chapters using the provided helper function.\n",
    "#    This function takes the path to the PDF and returns a list of Document objects, each representing a chapter.\n",
    "chapters = split_into_chapters(hp_pdf_path)\n",
    "\n",
    "# 2. Clean up the text in each chapter by replacing unwanted characters (e.g., '\\t') with spaces.\n",
    "#    This ensures the text is consistent and easier to process downstream.\n",
    "chapters = replace_t_with_space(chapters)\n",
    "\n",
    "# 3. Print the number of chapters extracted to verify the result.\n",
    "print(len(chapters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74e434d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "📄 document_cleaned length: 221\n",
      "abc\n",
      "Found 2 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 13 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 1 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 12 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 15 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 1 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 13 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 12 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 1 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 1 quotes on one doc\n",
      "Found 14 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 15 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 12 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 14 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 13 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "✅ Quotes Extracted: 1299\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import helper_functions\n",
    "importlib.reload(helper_functions)\n",
    "\n",
    "# --- Load and Preprocess the PDF, then Extract Quotes ---\n",
    "\n",
    "# 1. Load the PDF\n",
    "loader = PyPDFLoader(hp_pdf_path)\n",
    "document = loader.load()\n",
    "\n",
    "# 2. Clean the document (remove tabs)\n",
    "document_cleaned = helper_functions.replace_t_with_space(document)\n",
    "print(\"📄 document_cleaned length:\", len(document_cleaned))\n",
    "\n",
    "# 3. Extract quotes as Documents\n",
    "book_quotes_list = helper_functions.extract_book_quotes_as_documents(document_cleaned)\n",
    "\n",
    "print(\"✅ Quotes Extracted:\", len(book_quotes_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cf0e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Summarization Prompt Template for LLM-based Summarization ---\n",
    "\n",
    "# Define the template string for summarization.\n",
    "# This template instructs the language model to write an extensive summary of the provided text.\n",
    "summarization_prompt_template = \"\"\"Write an extensive summary of the following:\n",
    "\n",
    "{text}\n",
    "\n",
    "SUMMARY:\"\"\"\n",
    "\n",
    "# Create a PromptTemplate object using the template string.\n",
    "# The input variable \"text\" will be replaced with the content to summarize.\n",
    "summarization_prompt = PromptTemplate(\n",
    "    template=summarization_prompt_template,\n",
    "    input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e40db0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "importlib.reload(helper_functions)\n",
    "\n",
    "\n",
    "# Gemini doesn't have an official tokenizer, so we use a rough heuristic\n",
    "def num_tokens_from_string(string: str, model_name: str) -> int:\n",
    "    return len(string) // 4  # Rough estimate: 4 characters per token\n",
    "\n",
    "def create_chapter_summary(chapter):\n",
    "    \"\"\"\n",
    "    Creates a summary of a chapter using a large language model (LLM).\n",
    "\n",
    "    Args:\n",
    "        chapter: A Document object representing the chapter to summarize.\n",
    "\n",
    "    Returns:\n",
    "        A Document object containing the summary of the chapter.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the text content from the chapter\n",
    "    chapter_txt = chapter.page_content\n",
    "\n",
    "    # Specify the LLM model and configuration\n",
    "    model_name = \"gemini-2.5-pro\"\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=model_name,\n",
    "        temperature=0,\n",
    "        convert_system_message_to_human=True\n",
    "    )\n",
    "    gpt_35_turbo_max_tokens = 16000  # Keep your original logic/variable name\n",
    "    verbose = False  # Set to True for more detailed output\n",
    "\n",
    "    # Calculate the number of tokens in the chapter text\n",
    "    num_tokens = num_tokens_from_string(chapter_txt, model_name)\n",
    "\n",
    "    # Choose the summarization chain type based on token count\n",
    "    if num_tokens < gpt_35_turbo_max_tokens:\n",
    "        # For shorter chapters, use the \"stuff\" chain type\n",
    "        chain = load_summarize_chain(\n",
    "            llm,\n",
    "            chain_type=\"stuff\",\n",
    "            prompt=summarization_prompt,\n",
    "            verbose=verbose\n",
    "        )\n",
    "    else:\n",
    "        # For longer chapters, use the \"map_reduce\" chain type\n",
    "        chain = load_summarize_chain(\n",
    "            llm,\n",
    "            chain_type=\"map_reduce\",\n",
    "            map_prompt=summarization_prompt,\n",
    "            combine_prompt=summarization_prompt,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "    # Start timer to measure summarizatime\n",
    "    start_time = monotonic()\n",
    "\n",
    "    # Create a Document object for the chapter\n",
    "    doc_chapter = Document(page_content=chapter_txt)\n",
    "\n",
    "    # Generate the summary using the selected chain\n",
    "    summary_result = chain.invoke([doc_chapter])\n",
    "\n",
    "    # Print chain type and execution time for reference\n",
    "    print(f\"Chain type: {chain.__class__.__name__}\")\n",
    "    print(f\"Run time: {monotonic() - start_time}\")\n",
    "\n",
    "    # Clean up the summary text (remove double newlines, etc.)\n",
    "    summary_text = helper_functions.replace_double_lines_with_one_line(summary_result[\"output_text\"])\n",
    "\n",
    "    # Create a Document object for the summary, preserving chapter metadata\n",
    "    doc_summary = Document(page_content=summary_text, metadata=chapter.metadata)\n",
    "\n",
    "    return doc_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8de58e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chapter 14 of 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain type: StuffDocumentsChain\n",
      "Run time: 38.26162130001467\n",
      "Checkpoint saved at chapter 14\n",
      "Processing chapter 15 of 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain type: StuffDocumentsChain\n",
      "Run time: 42.91590920003364\n",
      "Checkpoint saved at chapter 15\n",
      "Processing chapter 16 of 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain type: StuffDocumentsChain\n",
      "Run time: 46.75584999995772\n",
      "Checkpoint saved at chapter 16\n",
      "Processing chapter 17 of 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain type: StuffDocumentsChain\n",
      "Run time: 45.50914509996073\n",
      "Checkpoint saved at chapter 17\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Where to save the checkpoint\n",
    "checkpoint_path = Path(\"chapter_summaries_checkpoint.pkl\")\n",
    "\n",
    "# Load checkpoint if exists\n",
    "if checkpoint_path.exists():\n",
    "    with open(checkpoint_path, \"rb\") as f:\n",
    "        chapter_summaries = pickle.load(f)\n",
    "    start_index = len(chapter_summaries)\n",
    "else:\n",
    "    chapter_summaries = []\n",
    "    start_index = 0\n",
    "\n",
    "# Set how many chapters to process per run\n",
    "max_chapters_per_run = 10\n",
    "total_chapters = 17  # or len(chapters), but you specified 1–17\n",
    "end_index = min(start_index + max_chapters_per_run, total_chapters)\n",
    "\n",
    "# Process chapters from start_index to end_index - 1\n",
    "for i in range(start_index, end_index):\n",
    "    chapter = chapters[i]\n",
    "    print(f\"Processing chapter {i+1} of {total_chapters}\")\n",
    "    \n",
    "    try:\n",
    "        summary = create_chapter_summary(chapter)\n",
    "        chapter_summaries.append(summary)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chapter {i+1}: {e}\")\n",
    "        break  # or continue to skip errors\n",
    "\n",
    "    # Save checkpoint after each chapter for safety\n",
    "    with open(checkpoint_path, \"wb\") as f:\n",
    "        pickle.dump(chapter_summaries, f)\n",
    "    print(f\"Checkpoint saved at chapter {i+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80a756e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def encode_book(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Encodes a PDF book into a FAISS vector store using HuggingFace embeddings.\n",
    "\n",
    "    Args:\n",
    "        path (str): The path to the PDF file.\n",
    "        chunk_size (int): The desired size of each text chunk.\n",
    "        chunk_overlap (int): The amount of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        FAISS: A FAISS vector store containing the encoded book content.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Load the PDF document using PyPDFLoader\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # 2. Split the document into chunks for embedding\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    # 3. Clean up the text chunks (replace unwanted characters)\n",
    "    cleaned_texts = replace_t_with_space(texts)\n",
    "\n",
    "    # 4. Create HuggingFace embeddings and encode the cleaned text chunks into a FAISS vector store\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
    "\n",
    "    # 5. Return the vector store\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69f1deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_chapter_summaries(chapter_summaries):\n",
    "    \"\"\"\n",
    "    Encodes a list of chapter summaries into a FAISS vector store using HuggingFace embeddings.\n",
    "\n",
    "    Args:\n",
    "        chapter_summaries (list): A list of Document objects representing the chapter summaries.\n",
    "\n",
    "    Returns:\n",
    "        FAISS: A FAISS vector store containing the encoded chapter summaries.\n",
    "    \"\"\"\n",
    "    # Create HuggingFace embeddings instance\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Encode the chapter summaries into a FAISS vector store\n",
    "    chapter_summaries_vectorstore = FAISS.from_documents(chapter_summaries, embeddings)\n",
    "    \n",
    "    # Return the vector store\n",
    "    return chapter_summaries_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "516ef92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_quotes(book_quotes_list):\n",
    "    \"\"\"\n",
    "    Encodes a list of book quotes into a FAISS vector store using HuggingFace embeddings.\n",
    "\n",
    "    Args:\n",
    "        book_quotes_list (list): A list of Document objects, each representing a quote from the book.\n",
    "\n",
    "    Returns:\n",
    "        FAISS: A FAISS vector store containing the encoded book quotes.\n",
    "    \"\"\"\n",
    "    # Create HuggingFace embeddings instance\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Encode the book quotes into a FAISS vector store\n",
    "    quotes_vectorstore = FAISS.from_documents(book_quotes_list, embeddings)\n",
    "    \n",
    "    # Return the vector store\n",
    "    return quotes_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43bea260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NAFEES J\\AppData\\Local\\Temp\\ipykernel_23040\\3118181819.py:11: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# --- Create or Load Vector Stores for Book Chunks, Chapter Summaries, and Book Quotes ---\n",
    "\n",
    "# Check if the vector stores already exist on disk\n",
    "if (\n",
    "    os.path.exists(\"chunks_vector_store\") and\n",
    "    os.path.exists(\"chapter_summaries_vector_store\") and\n",
    "    os.path.exists(\"book_quotes_vectorstore\")\n",
    "):\n",
    "    # If vector stores exist, load them using HuggingFace embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    chunks_vector_store = FAISS.load_local(\n",
    "        \"chunks_vector_store\", embeddings, allow_dangerous_deserialization=True\n",
    "    )\n",
    "    chapter_summaries_vector_store = FAISS.load_local(\n",
    "        \"chapter_summaries_vector_store\", embeddings, allow_dangerous_deserialization=True\n",
    "    )\n",
    "    book_quotes_vectorstore = FAISS.load_local(\n",
    "        \"book_quotes_vectorstore\", embeddings, allow_dangerous_deserialization=True\n",
    "    )\n",
    "else:\n",
    "    print(\"not\")\n",
    "    # If vector stores do not exist, encode and save them\n",
    "\n",
    "    # 1. Encode the book into a vector store of chunks\n",
    "    chunks_vector_store = encode_book(hp_pdf_path, chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "    # 2. Encode the chapter summaries into a vector store\n",
    "    chapter_summaries_vector_store = encode_chapter_summaries(chapter_summaries)\n",
    "\n",
    "    # 3. Encode the book quotes into a vector store\n",
    "    book_quotes_vectorstore = encode_quotes(book_quotes_list)\n",
    "\n",
    "    # 4. Save the vector stores to disk for future use\n",
    "    chunks_vector_store.save_local(\"chunks_vector_store\")\n",
    "    chapter_summaries_vector_store.save_local(\"chapter_summaries_vector_store\")\n",
    "    book_quotes_vectorstore.save_local(\"book_quotes_vectorstore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a4cf5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create Query Retrievers from Vector Stores ---\n",
    "\n",
    "# The following retrievers are used to fetch relevant documents from the vector stores\n",
    "# based on a query. The number of results returned can be controlled via the 'k' parameter.\n",
    "\n",
    "# Retriever for book chunks (returns the top 1 most relevant chunk)\n",
    "chunks_query_retriever = chunks_vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# Retriever for chapter summaries (returns the top 1 most relevant summary)\n",
    "chapter_summaries_query_retriever = chapter_summaries_vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# Retriever for book quotes (returns the top 10 most relevant quotes)\n",
    "book_quotes_query_retriever = book_quotes_vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06dd66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import helper_functions\n",
    "importlib.reload(helper_functions)\n",
    "def retrieve_context_per_question(state):\n",
    "    \"\"\"\n",
    "    Retrieves relevant context for a given question by aggregating content from:\n",
    "    - Book chunks\n",
    "    - Chapter summaries\n",
    "    - Book quotes\n",
    "\n",
    "    Args:\n",
    "        state (dict): A dictionary containing the question to answer, with key \"question\".\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with keys:\n",
    "            - \"context\": Aggregated context string from all sources.\n",
    "            - \"question\": The original question.\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieve relevant book chunks\n",
    "    print(\"Retrieving relevant chunks...\")\n",
    "    docs = chunks_query_retriever.get_relevant_documents(question)\n",
    "    context = \" \".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Retrieve relevant chapter summaries\n",
    "    print(\"Retrieving relevant chapter summaries...\")\n",
    "    docs_summaries = chapter_summaries_query_retriever.get_relevant_documents(question)\n",
    "    context_summaries = \" \".join(\n",
    "        f\"{doc.page_content} (Chapter {doc.metadata['chapter']})\" for doc in docs_summaries\n",
    "    )\n",
    "\n",
    "    # Retrieve relevant book quotes\n",
    "    print(\"Retrieving relevant book quotes...\")\n",
    "    docs_book_quotes = book_quotes_query_retriever.get_relevant_documents(question)\n",
    "    book_quotes = \" \".join(doc.page_content for doc in docs_book_quotes)\n",
    "    # Aggregate all contexts and escape problematic characters\n",
    "    all_contexts = context + context_summaries + book_quotes\n",
    "    all_contexts =helper_functions.escape_quotes(all_contexts)\n",
    "\n",
    "    return {\"context\": all_contexts, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098c34d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "importlib.reload(helper_functions)\n",
    "# --- Prompt template ---\n",
    "keep_only_relevant_content_prompt_template = \"\"\"\n",
    "You receive a query: {query} and retrieved documents: {retrieved_documents} from a vector store.\n",
    "You need to filter out all the non relevant information that doesn't supply important information regarding the {query}.\n",
    "Your goal is just to filter out the non relevant information.\n",
    "You can remove parts of sentences that are not relevant to the query or remove whole sentences that are not relevant to the query.\n",
    "DO NOT ADD ANY NEW INFORMATION THAT IS NOT IN THE RETRIEVED DOCUMENTS.\n",
    "Output the filtered relevant content.\n",
    "\"\"\"\n",
    "\n",
    "# --- Output schema ---\n",
    "class KeepRelevantContent(BaseModel):\n",
    "    relevant_content: str = Field(\n",
    "        description=\"The relevant content from the retrieved documents that is relevant to the query.\"\n",
    "    )\n",
    "\n",
    "# --- Prompt + chain ---\n",
    "keep_only_relevant_content_prompt = PromptTemplate(\n",
    "    template=keep_only_relevant_content_prompt_template,\n",
    "    input_variables=[\"query\", \"retrieved_documents\"],\n",
    ")\n",
    "\n",
    "keep_only_relevant_content_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    temperature=0,\n",
    "    convert_system_message_to_human=True\n",
    ")\n",
    "\n",
    "keep_only_relevant_content_chain = (\n",
    "    keep_only_relevant_content_prompt\n",
    "    | keep_only_relevant_content_llm.with_structured_output(KeepRelevantContent)\n",
    ")\n",
    "\n",
    "# --- Filtering function ---\n",
    "def keep_only_relevant_content(state):\n",
    "    \"\"\"\n",
    "    Filters and keeps only the relevant content from the retrieved documents that is relevant to the query.\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    input_data = {\n",
    "        \"query\": question,\n",
    "        \"retrieved_documents\": context\n",
    "    }\n",
    "\n",
    "    print(\"Keeping only the relevant content...\")\n",
    "    pprint(\"--------------------\")\n",
    "    output = keep_only_relevant_content_chain.invoke(input_data)\n",
    "    relevant_content = output.relevant_content\n",
    "    relevant_content = \"\".join(relevant_content)\n",
    "\n",
    "    # Escape quotes if needed (define escape_quotes function elsewhere if used)\n",
    "    relevant_content = helper_functions.escape_quotes(relevant_content)\n",
    "\n",
    "    return {\n",
    "        \"relevant_context\": relevant_content,\n",
    "        \"context\": context,\n",
    "        \"question\": question\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c64f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM-based Function to Rewrite a Question for Better Vectorstore Retrieval ---\n",
    "\n",
    "class RewriteQuestion(BaseModel):\n",
    "    \"\"\"\n",
    "    Output schema for the rewritten question.\n",
    "    \"\"\"\n",
    "    rewritten_question: str = Field(\n",
    "        description=\"The improved question optimized for vectorstore retrieval.\"\n",
    "    )\n",
    "    explanation: str = Field(\n",
    "        description=\"The explanation of the rewritten question.\"\n",
    "    )\n",
    "\n",
    "# Create a JSON parser for the output schema\n",
    "rewrite_question_string_parser = JsonOutputParser(pydantic_object=RewriteQuestion)\n",
    "\n",
    "# Initialize the LLM for rewriting questions\n",
    "rewrite_llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"llama3-70b-8192\",\n",
    "    groq_api_key=groq_api_key,\n",
    "    max_tokens=4000\n",
    ")\n",
    "\n",
    "# Define the prompt template for question rewriting\n",
    "rewrite_prompt_template = \"\"\"\n",
    "You are a question re-writer that converts an input question to a better version optimized for vectorstore retrieval.\n",
    "Analyze the input question {question}  like it maybe a trick way to answer a simple question and try to reason about the underlying semantic intent / meaning.\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "# Create the prompt object\n",
    "rewrite_prompt = PromptTemplate(\n",
    "    template=rewrite_prompt_template,\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": rewrite_question_string_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Combine prompt, LLM, and parser into a chain\n",
    "question_rewriter = rewrite_prompt | rewrite_llm | rewrite_question_string_parser\n",
    "\n",
    "def rewrite_question(state):\n",
    "    \"\"\"\n",
    "    Rewrites the given question using the LLM to optimize it for vectorstore retrieval.\n",
    "\n",
    "    Args:\n",
    "        state (dict): A dictionary containing the question to rewrite, with key \"question\".\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the rewritten question under the key \"question\".\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    print(\"Rewriting the question...\")\n",
    "    result = question_rewriter.invoke({\"question\": question})\n",
    "    new_question = result[\"rewritten_question\"]\n",
    "    return {\"question\": new_question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a028992d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def answer_question_from_context(state):\\n    \"\"\"\\n    Answers a question from a given context using chain-of-thought reasoning.\\n\\n    Args:\\n        state (dict): A dictionary containing:\\n            - \"question\": The query question.\\n            - \"context\" or \"aggregated_context\": The context to answer the question from.\\n\\n    Returns:\\n        dict: A dictionary containing:\\n            - \"answer\": The answer to the question from the context.\\n            - \"context\": The context used.\\n            - \"question\": The original question.\\n    \"\"\"\\n    # Use \\'aggregated_context\\' if available, otherwise fall back to \\'context\\'\\n    question = state[\"question\"]\\n    context = state[\"aggregated_context\"] if \"aggregated_context\" in state else state[\"context\"]\\n\\n    input_data = {\\n        \"question\": question,\\n        \"context\": context\\n    }\\n\\n    print(\"Answering the question from the retrieved context...\")\\n\\n    # Invoke the LLM chain to get the answer\\n    output = question_answer_from_context_cot_chain.invoke(input_data)\\n    answer = output.answer_based_on_content\\n    print(f\\'answer before checking hallucination: {answer}\\')\\n\\n    return {\\n        \"answer\": answer,\\n        \"context\": context,\\n        \"question\": question\\n    }'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- LLM-based Function to Answer a Question from Context Using Chain-of-Thought Reasoning ---\n",
    "\n",
    "# Define the output schema for the answer\n",
    "class QuestionAnswerFromContext(BaseModel):\n",
    "    answer_based_on_content: str = Field(\n",
    "        description=\"Generates an answer to a query based on a given context.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Initialize the LLM for answering questions with chain-of-thought reasoning\n",
    "question_answer_from_context_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    temperature=0,\n",
    "    convert_system_message_to_human=True\n",
    ")\n",
    "# Define the prompt template with chain-of-thought examples and instructions\n",
    "question_answer_cot_prompt_template = \"\"\" \n",
    "Examples of Chain-of-Thought Reasoning\n",
    "\n",
    "Example 1\n",
    "\n",
    "Context: Mary is taller than Jane. Jane is shorter than Tom. Tom is the same height as David.\n",
    "Question: Who is the tallest person?\n",
    "Reasoning Chain:\n",
    "The context tells us Mary is taller than Jane\n",
    "It also says Jane is shorter than Tom\n",
    "And Tom is the same height as David\n",
    "So the order from tallest to shortest is: Mary, Tom/David, Jane\n",
    "Therefore, Mary must be the tallest person\n",
    "\n",
    "Example 2\n",
    "Context: Harry was reading a book about magic spells. One spell allowed the caster to turn a person into an animal for a short time. Another spell could levitate objects.\n",
    " A third spell created a bright light at the end of the caster's wand.\n",
    "Question: Based on the context, if Harry cast these spells, what could he do?\n",
    "Reasoning Chain:\n",
    "The context describes three different magic spells\n",
    "The first spell allows turning a person into an animal temporarily\n",
    "The second spell can levitate or float objects\n",
    "The third spell creates a bright light\n",
    "If Harry cast these spells, he could turn someone into an animal for a while, make objects float, and create a bright light source\n",
    "So based on the context, if Harry cast these spells he could transform people, levitate things, and illuminate an area\n",
    "Instructions.\n",
    "\n",
    "Example 3 \n",
    "Context: Harry Potter woke up on his birthday to find a present at the end of his bed. He excitedly opened it to reveal a Nimbus 2000 broomstick.\n",
    "Question: Why did Harry receive a broomstick for his birthday?\n",
    "Reasoning Chain:\n",
    "The context states that Harry Potter woke up on his birthday and received a present - a Nimbus 2000 broomstick.\n",
    "However, the context does not provide any information about why he received that specific present or who gave it to him.\n",
    "There are no details about Harry's interests, hobbies, or the person who gifted him the broomstick.\n",
    "Without any additional context about Harry's background or the gift-giver's motivations, there is no way to determine the reason he received a broomstick as a birthday present.\n",
    "\n",
    "For the question below, provide your answer by first showing your step-by-step reasoning process, breaking down the problem into a chain of thought before arriving at the final answer,\n",
    " just like in the previous examples.\n",
    "Context\n",
    "{context}\n",
    "Question\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt object\n",
    "question_answer_from_context_cot_prompt = PromptTemplate(\n",
    "    template=question_answer_cot_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"],\n",
    ")\n",
    "\n",
    "# Combine the prompt and LLM into a chain with structured output\n",
    "question_answer_from_context_cot_chain = (\n",
    "    question_answer_from_context_cot_prompt\n",
    "    | question_answer_from_context_llm.with_structured_output(QuestionAnswerFromContext)\n",
    ")\n",
    "\n",
    "'''def answer_question_from_context(state):\n",
    "    \"\"\"\n",
    "    Answers a question from a given context using chain-of-thought reasoning.\n",
    "\n",
    "    Args:\n",
    "        state (dict): A dictionary containing:\n",
    "            - \"question\": The query question.\n",
    "            - \"context\" or \"aggregated_context\": The context to answer the question from.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - \"answer\": The answer to the question from the context.\n",
    "            - \"context\": The context used.\n",
    "            - \"question\": The original question.\n",
    "    \"\"\"\n",
    "    # Use 'aggregated_context' if available, otherwise fall back to 'context'\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"aggregated_context\"] if \"aggregated_context\" in state else state[\"context\"]\n",
    "\n",
    "    input_data = {\n",
    "        \"question\": question,\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "    print(\"Answering the question from the retrieved context...\")\n",
    "\n",
    "    # Invoke the LLM chain to get the answer\n",
    "    output = question_answer_from_context_cot_chain.invoke(input_data)\n",
    "    answer = output.answer_based_on_content\n",
    "    print(f'answer before checking hallucination: {answer}')\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"context\": context,\n",
    "        \"question\": question\n",
    "    }'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cc6ba6",
   "metadata": {},
   "source": [
    "LLM based function to determine if retrieved content is relevant to the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c64d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM-based Function to Determine Relevance of Retrieved Content ---\n",
    "\n",
    "# Prompt template for checking if the retrieved context is relevant to the query\n",
    "is_relevant_content_prompt_template = \"\"\"\n",
    "You receive a query: {query} and a context: {context} retrieved from a vector store. \n",
    "You need to determine if the document is relevant to the query. \n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "# Output schema for the relevance check\n",
    "class Relevance(BaseModel):\n",
    "    is_relevant: bool = Field(description=\"Whether the document is relevant to the query.\")\n",
    "    explanation: str = Field(description=\"An explanation of why the document is relevant or not.\")\n",
    "\n",
    "# JSON parser for the output schema\n",
    "is_relevant_json_parser = JsonOutputParser(pydantic_object=Relevance)\n",
    "\n",
    "# Initialize the LLM for relevance checking\n",
    "is_relevant_llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"llama3-70b-8192\",\n",
    "    groq_api_key=groq_api_key,\n",
    "    max_tokens=4000\n",
    ")\n",
    "\n",
    "# Create the prompt object for the LLM\n",
    "is_relevant_content_prompt = PromptTemplate(\n",
    "    template=is_relevant_content_prompt_template,\n",
    "    input_variables=[\"query\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": is_relevant_json_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Combine prompt, LLM, and parser into a chain\n",
    "is_relevant_content_chain = is_relevant_content_prompt | is_relevant_llm | is_relevant_json_parser\n",
    "def is_relevant_content(state):\n",
    "    \"\"\"\n",
    "    Determines if the retrieved context is relevant to the query.\n",
    "\n",
    "    Args:\n",
    "        state (dict): A dictionary containing:\n",
    "            - \"question\": The query question.\n",
    "            - \"context\": The retrieved context to check for relevance.\n",
    "\n",
    "    Returns:\n",
    "        str: \"relevant\" if the context is relevant, \"not relevant\" otherwise.\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    input_data = {\n",
    "        \"query\": question,\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "    # Invoke the LLM chain to determine if the document is relevant\n",
    "    output = is_relevant_content_chain.invoke(input_data)\n",
    "    print(\"Determining if the document is relevant...\")\n",
    "    if output[\"is_relevant\"]:\n",
    "        print(\"The document is relevant.\")\n",
    "        return \"relevant\"\n",
    "    else:\n",
    "        print(\"The document is not relevant.\")\n",
    "        return \"not relevant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dad19f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(state):\n",
    "    \"\"\"\n",
    "    Full RAG + CoT answer generation using Gemini 2.5 Pro.\n",
    "    - rewrite_question: function\n",
    "    - retrieve_context_per_question: function\n",
    "    - keep_only_relevant_content: function\n",
    "    - question_answer_from_context_cot_chain: LangChain CoT chain\n",
    "    \"\"\"\n",
    "\n",
    "    # ✅ Step 1: Rewrite the question\n",
    "    rewritten_state = rewrite_question(state)\n",
    "    question = rewritten_state[\"question\"]\n",
    "    print(\"🔁 Rewritten question:\", question)\n",
    "\n",
    "    # ✅ Step 2: Retrieve context\n",
    "    retrieval_output = retrieve_context_per_question({\"question\": question})\n",
    "    context = retrieval_output[\"context\"]\n",
    "    print(\"📚 Retrieved context:\", context[:300], \"...\")\n",
    "\n",
    "    # ✅ Step 3: Filter relevant content\n",
    "    filtered_output = keep_only_relevant_content({\n",
    "        \"question\": question,\n",
    "        \"context\": context\n",
    "    })\n",
    "    relevant_context = filtered_output[\"relevant_context\"]\n",
    "    print(\"🧹 Filtered context:\", relevant_context[:300], \"...\")\n",
    "\n",
    "    # ✅ Step 4: Relevance check before answer generation\n",
    "    relevance_state = {\n",
    "        \"question\": question,\n",
    "        \"context\": relevant_context\n",
    "    }\n",
    "    relevance_result = is_relevant_content(relevance_state)\n",
    "    if relevance_result == \"not relevant\":\n",
    "        print(\"❌ Skipping answer generation: Context not relevant.\")\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"context\": relevant_context,\n",
    "            \"answer\": \"The retrieved context is not relevant enough to answer this question.\"\n",
    "        }\n",
    "\n",
    "    # ✅ Step 5: Use Gemini CoT chain\n",
    "    input_data = {\n",
    "        \"question\": question,\n",
    "        \"context\": relevant_context\n",
    "    }\n",
    "\n",
    "    print(\"🤖 Generating answer using CoT Gemini chain...\")\n",
    "    output = question_answer_from_context_cot_chain.invoke(input_data)\n",
    "    answer = output.answer_based_on_content\n",
    "    print(f'✅ Final answer: {answer}')\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"context\": relevant_context,\n",
    "        \"answer\": answer\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88336403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewriting the question...\n",
      "🔁 Rewritten question: What subject does the professor who assisted the antagonist teach?\n",
      "Retrieving relevant chunks...\n",
      "Retrieving relevant chapter summaries...\n",
      "Retrieving relevant book quotes...\n",
      "📚 Retrieved context: waving your wand and saying a few funny words.\n",
      "      They had to study the night skies through their telescopes every\n",
      "Wednesday at midnight and learn the names of different stars and the movements\n",
      "of the planets. Three times a week they went out to the greenhouses behind the\n",
      "castle to study Herbolog ...\n",
      "Keeping only the relevant content...\n",
      "'--------------------'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Filtered context: The chapter opens with Harry entering the final chamber, expecting to face Professor Snape. Instead, he is stunned to find the timid, stuttering Professor Quirrell. Quirrell reveals that he, not Snape, has been the one trying to steal the Stone and harm Harry all along. This leads to further revelat ...\n",
      "Determining if the document is relevant...\n",
      "The document is relevant.\n",
      "🤖 Generating answer using CoT Gemini chain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final answer: Reasoning Chain:\n",
      "The user is asking about the subject taught by the professor who assisted the antagonist.\n",
      "The context identifies the antagonist as Lord Voldemort and the professor who assisted him as Professor Quirrell.\n",
      "I need to find what subject Professor Quirrell teaches.\n",
      "I will review the context to find this information.\n",
      "After reviewing the context, I can see that it does not mention the specific subject that Professor Quirrell teaches.\n",
      "\n",
      "Final Answer: The provided text does not contain information about the subject that Professor Quirrell teaches.\n",
      "Rewriting the question...\n",
      "🔁 Rewritten question: What is the name of the professor who teaches Defense Against the Dark Arts?\n",
      "Retrieving relevant chunks...\n",
      "Retrieving relevant chapter summaries...\n",
      "Retrieving relevant book quotes...\n",
      "📚 Retrieved context: waving your wand and saying a few funny words.\n",
      "      They had to study the night skies through their telescopes every\n",
      "Wednesday at midnight and learn the names of different stars and the movements\n",
      "of the planets. Three times a week they went out to the greenhouses behind the\n",
      "castle to study Herbolog ...\n",
      "Keeping only the relevant content...\n",
      "'--------------------'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Filtered context: “Potter, I know what I’m talking about, D-Defense Against the D-D-Dark Arts,” muttered Professor Quirrell, as though he’d rather not think about it. Professor Quirrell!” said Hagrid. “Harry, Professor Quirrell will be one of your teachers at Hogwarts. everyone knows he’s after Quirrell’s job. ...\n",
      "Determining if the document is relevant...\n",
      "The document is relevant.\n",
      "🤖 Generating answer using CoT Gemini chain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final answer: Reasoning Chain:\n",
      "The context includes a quote from Professor Quirrell where he says, “D-Defense Against the D-D-Dark Arts.”\n",
      "Hagrid then confirms that Professor Quirrell is a teacher at Hogwarts.\n",
      "Therefore, based on the context, the professor who teaches Defense Against the Dark Arts is Professor Quirrell.\n",
      "\n",
      "Answer:\n",
      "Professor Quirrell teaches Defense Against the Dark Arts.\n",
      "Rewriting the question...\n",
      "🔁 Rewritten question: What subject does the professor who assisted the antagonist teach?\n",
      "Retrieving relevant chunks...\n",
      "Retrieving relevant chapter summaries...\n",
      "Retrieving relevant book quotes...\n",
      "📚 Retrieved context: waving your wand and saying a few funny words.\n",
      "      They had to study the night skies through their telescopes every\n",
      "Wednesday at midnight and learn the names of different stars and the movements\n",
      "of the planets. Three times a week they went out to the greenhouses behind the\n",
      "castle to study Herbolog ...\n",
      "Keeping only the relevant content...\n",
      "'--------------------'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Filtered context: The chapter opens with Harry entering the final chamber, expecting to face Professor Snape. Instead, he is stunned to find the timid, stuttering Professor Quirrell, who greets him with a calm, sharp voice, completely devoid of his usual nervous twitch. Quirrell confesses everything, systematically d ...\n",
      "Determining if the document is relevant...\n",
      "The document is relevant.\n",
      "🤖 Generating answer using CoT Gemini chain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final answer: Reasoning Chain:\n",
      "The context reveals that Professor Quirrell is the one who assisted the antagonist, Lord Voldemort.\n",
      "The text mentions that Professor Quirrell is a teacher at Hogwarts.\n",
      "However, the provided context does not specify what subject Professor Quirrell teaches.\n",
      "Final Answer: The provided context does not mention the subject that Professor Quirrell teaches.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# This is your RAG function using Gemini\n",
    "def rag_interface(question):\n",
    "    state = {\"question\": question}\n",
    "    output = generate_answer(state)\n",
    "    return output[\"answer\"]\n",
    "\n",
    "# Launch a Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=rag_interface,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Ask a question about the book...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"📘 Book Q&A with Gemini\",\n",
    "    description=\"Ask any question about the book. Answers are generated using Gemini and retrieved from book chunks, chapter summaries, and quotes.\"\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
