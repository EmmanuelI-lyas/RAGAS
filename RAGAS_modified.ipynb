{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5e681ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- LangChain and LLM Imports ---\n",
    "from langchain_openai import ChatOpenAI \n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# --- Document Loading and Vector Store ---\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings \n",
    "\n",
    "# --- Prompting and Document Utilities ---\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "# --- Core and Output Parsers ---\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "# --- LangGraph for Workflow Graphs ---\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "from time import monotonic\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "# --- Datasets and Typing ---\n",
    "from datasets import Dataset\n",
    "from typing_extensions import TypedDict\n",
    "from IPython.display import display, Image\n",
    "from typing import List, TypedDict\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_correctness,\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    answer_similarity\n",
    ")\n",
    "from pathlib import Path\n",
    "\n",
    "import langgraph\n",
    "\n",
    "\n",
    "\n",
    "# --- Load environment variables (e.g., API keys) ---\n",
    "load_dotenv(dotenv_path=Path().resolve() / \".env\", override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a03651e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the OpenAI API key from environment variable (for use by OpenAI LLMs)\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# Retrieve the Groq API key from environment variable (for use by Groq LLMs)\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "440691f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the Harry Potter PDF file.\n",
    "# This variable will be used throughout the notebook for loading and processing the book.\n",
    "hp_pdf_path =r\"C:\\Users\\NAFEES J\\Downloads\\Harry Potter - Book 1 - The Sorcerers Stone.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6bea1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import split_into_chapters, replace_t_with_space\n",
    "# --- Split the PDF into chapters and preprocess the text ---\n",
    "\n",
    "# 1. Split the PDF into chapters using the provided helper function.\n",
    "#    This function takes the path to the PDF and returns a list of Document objects, each representing a chapter.\n",
    "chapters = split_into_chapters(hp_pdf_path)\n",
    "\n",
    "# 2. Clean up the text in each chapter by replacing unwanted characters (e.g., '\\t') with spaces.\n",
    "#    This ensures the text is consistent and easier to process downstream.\n",
    "chapters = replace_t_with_space(chapters)\n",
    "\n",
    "# 3. Print the number of chapters extracted to verify the result.\n",
    "print(len(chapters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74e434d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "📄 document_cleaned length: 221\n",
      "abc\n",
      "Found 2 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 13 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 1 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 12 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 15 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 1 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 13 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 12 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 1 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 1 quotes on one doc\n",
      "Found 14 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 2 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 15 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 12 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 14 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 3 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 8 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 13 quotes on one doc\n",
      "Found 9 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 11 quotes on one doc\n",
      "Found 10 quotes on one doc\n",
      "Found 5 quotes on one doc\n",
      "Found 7 quotes on one doc\n",
      "Found 4 quotes on one doc\n",
      "Found 6 quotes on one doc\n",
      "✅ Quotes Extracted: 1299\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import helper_functions\n",
    "importlib.reload(helper_functions)\n",
    "\n",
    "# --- Load and Preprocess the PDF, then Extract Quotes ---\n",
    "\n",
    "# 1. Load the PDF\n",
    "loader = PyPDFLoader(hp_pdf_path)\n",
    "document = loader.load()\n",
    "\n",
    "# 2. Clean the document (remove tabs)\n",
    "document_cleaned = helper_functions.replace_t_with_space(document)\n",
    "print(\"📄 document_cleaned length:\", len(document_cleaned))\n",
    "\n",
    "# 3. Extract quotes as Documents\n",
    "book_quotes_list = helper_functions.extract_book_quotes_as_documents(document_cleaned)\n",
    "\n",
    "print(\"✅ Quotes Extracted:\", len(book_quotes_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cf0e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Summarization Prompt Template for LLM-based Summarization ---\n",
    "\n",
    "# Define the template string for summarization.\n",
    "# This template instructs the language model to write an extensive summary of the provided text.\n",
    "summarization_prompt_template = \"\"\"Write an extensive summary of the following:\n",
    "\n",
    "{text}\n",
    "\n",
    "SUMMARY:\"\"\"\n",
    "\n",
    "# Create a PromptTemplate object using the template string.\n",
    "# The input variable \"text\" will be replaced with the content to summarize.\n",
    "summarization_prompt = PromptTemplate(\n",
    "    template=summarization_prompt_template,\n",
    "    input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e40db0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "importlib.reload(helper_functions)\n",
    "\n",
    "\n",
    "# Gemini doesn't have an official tokenizer, so we use a rough heuristic\n",
    "def num_tokens_from_string(string: str, model_name: str) -> int:\n",
    "    return len(string) // 4  # Rough estimate: 4 characters per token\n",
    "\n",
    "def create_chapter_summary(chapter):\n",
    "    \"\"\"\n",
    "    Creates a summary of a chapter using a large language model (LLM).\n",
    "\n",
    "    Args:\n",
    "        chapter: A Document object representing the chapter to summarize.\n",
    "\n",
    "    Returns:\n",
    "        A Document object containing the summary of the chapter.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the text content from the chapter\n",
    "    chapter_txt = chapter.page_content\n",
    "\n",
    "    # Specify the LLM model and configuration\n",
    "    model_name = \"gemini-2.5-pro\"\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=model_name,\n",
    "        temperature=0,\n",
    "        convert_system_message_to_human=True\n",
    "    )\n",
    "    gpt_35_turbo_max_tokens = 16000  # Keep your original logic/variable name\n",
    "    verbose = False  # Set to True for more detailed output\n",
    "\n",
    "    # Calculate the number of tokens in the chapter text\n",
    "    num_tokens = num_tokens_from_string(chapter_txt, model_name)\n",
    "\n",
    "    # Choose the summarization chain type based on token count\n",
    "    if num_tokens < gpt_35_turbo_max_tokens:\n",
    "        # For shorter chapters, use the \"stuff\" chain type\n",
    "        chain = load_summarize_chain(\n",
    "            llm,\n",
    "            chain_type=\"stuff\",\n",
    "            prompt=summarization_prompt,\n",
    "            verbose=verbose\n",
    "        )\n",
    "    else:\n",
    "        # For longer chapters, use the \"map_reduce\" chain type\n",
    "        chain = load_summarize_chain(\n",
    "            llm,\n",
    "            chain_type=\"map_reduce\",\n",
    "            map_prompt=summarization_prompt,\n",
    "            combine_prompt=summarization_prompt,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "    # Start timer to measure summarizatime\n",
    "    start_time = monotonic()\n",
    "\n",
    "    # Create a Document object for the chapter\n",
    "    doc_chapter = Document(page_content=chapter_txt)\n",
    "\n",
    "    # Generate the summary using the selected chain\n",
    "    summary_result = chain.invoke([doc_chapter])\n",
    "\n",
    "    # Print chain type and execution time for reference\n",
    "    print(f\"Chain type: {chain.__class__.__name__}\")\n",
    "    print(f\"Run time: {monotonic() - start_time}\")\n",
    "\n",
    "    # Clean up the summary text (remove double newlines, etc.)\n",
    "    summary_text = helper_functions.replace_double_lines_with_one_line(summary_result[\"output_text\"])\n",
    "\n",
    "    # Create a Document object for the summary, preserving chapter metadata\n",
    "    doc_summary = Document(page_content=summary_text, metadata=chapter.metadata)\n",
    "\n",
    "    return doc_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8de58e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chapter 14 of 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain type: StuffDocumentsChain\n",
      "Run time: 38.26162130001467\n",
      "Checkpoint saved at chapter 14\n",
      "Processing chapter 15 of 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain type: StuffDocumentsChain\n",
      "Run time: 42.91590920003364\n",
      "Checkpoint saved at chapter 15\n",
      "Processing chapter 16 of 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain type: StuffDocumentsChain\n",
      "Run time: 46.75584999995772\n",
      "Checkpoint saved at chapter 16\n",
      "Processing chapter 17 of 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain type: StuffDocumentsChain\n",
      "Run time: 45.50914509996073\n",
      "Checkpoint saved at chapter 17\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Where to save the checkpoint\n",
    "checkpoint_path = Path(\"chapter_summaries_checkpoint.pkl\")\n",
    "\n",
    "# Load checkpoint if exists\n",
    "if checkpoint_path.exists():\n",
    "    with open(checkpoint_path, \"rb\") as f:\n",
    "        chapter_summaries = pickle.load(f)\n",
    "    start_index = len(chapter_summaries)\n",
    "else:\n",
    "    chapter_summaries = []\n",
    "    start_index = 0\n",
    "\n",
    "# Set how many chapters to process per run\n",
    "max_chapters_per_run = 10\n",
    "total_chapters = 17  # or len(chapters), but you specified 1–17\n",
    "end_index = min(start_index + max_chapters_per_run, total_chapters)\n",
    "\n",
    "# Process chapters from start_index to end_index - 1\n",
    "for i in range(start_index, end_index):\n",
    "    chapter = chapters[i]\n",
    "    print(f\"Processing chapter {i+1} of {total_chapters}\")\n",
    "    \n",
    "    try:\n",
    "        summary = create_chapter_summary(chapter)\n",
    "        chapter_summaries.append(summary)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chapter {i+1}: {e}\")\n",
    "        break  # or continue to skip errors\n",
    "\n",
    "    # Save checkpoint after each chapter for safety\n",
    "    with open(checkpoint_path, \"wb\") as f:\n",
    "        pickle.dump(chapter_summaries, f)\n",
    "    print(f\"Checkpoint saved at chapter {i+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80a756e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def encode_book(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Encodes a PDF book into a FAISS vector store using HuggingFace embeddings.\n",
    "\n",
    "    Args:\n",
    "        path (str): The path to the PDF file.\n",
    "        chunk_size (int): The desired size of each text chunk.\n",
    "        chunk_overlap (int): The amount of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        FAISS: A FAISS vector store containing the encoded book content.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Load the PDF document using PyPDFLoader\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # 2. Split the document into chunks for embedding\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    # 3. Clean up the text chunks (replace unwanted characters)\n",
    "    cleaned_texts = replace_t_with_space(texts)\n",
    "\n",
    "    # 4. Create HuggingFace embeddings and encode the cleaned text chunks into a FAISS vector store\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
    "\n",
    "    # 5. Return the vector store\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69f1deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_chapter_summaries(chapter_summaries):\n",
    "    \"\"\"\n",
    "    Encodes a list of chapter summaries into a FAISS vector store using HuggingFace embeddings.\n",
    "\n",
    "    Args:\n",
    "        chapter_summaries (list): A list of Document objects representing the chapter summaries.\n",
    "\n",
    "    Returns:\n",
    "        FAISS: A FAISS vector store containing the encoded chapter summaries.\n",
    "    \"\"\"\n",
    "    # Create HuggingFace embeddings instance\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Encode the chapter summaries into a FAISS vector store\n",
    "    chapter_summaries_vectorstore = FAISS.from_documents(chapter_summaries, embeddings)\n",
    "    \n",
    "    # Return the vector store\n",
    "    return chapter_summaries_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "516ef92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_quotes(book_quotes_list):\n",
    "    \"\"\"\n",
    "    Encodes a list of book quotes into a FAISS vector store using HuggingFace embeddings.\n",
    "\n",
    "    Args:\n",
    "        book_quotes_list (list): A list of Document objects, each representing a quote from the book.\n",
    "\n",
    "    Returns:\n",
    "        FAISS: A FAISS vector store containing the encoded book quotes.\n",
    "    \"\"\"\n",
    "    # Create HuggingFace embeddings instance\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Encode the book quotes into a FAISS vector store\n",
    "    quotes_vectorstore = FAISS.from_documents(book_quotes_list, embeddings)\n",
    "    \n",
    "    # Return the vector store\n",
    "    return quotes_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43bea260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NAFEES J\\AppData\\Local\\Temp\\ipykernel_23040\\3118181819.py:11: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# --- Create or Load Vector Stores for Book Chunks, Chapter Summaries, and Book Quotes ---\n",
    "\n",
    "# Check if the vector stores already exist on disk\n",
    "if (\n",
    "    os.path.exists(\"chunks_vector_store\") and\n",
    "    os.path.exists(\"chapter_summaries_vector_store\") and\n",
    "    os.path.exists(\"book_quotes_vectorstore\")\n",
    "):\n",
    "    # If vector stores exist, load them using HuggingFace embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    chunks_vector_store = FAISS.load_local(\n",
    "        \"chunks_vector_store\", embeddings, allow_dangerous_deserialization=True\n",
    "    )\n",
    "    chapter_summaries_vector_store = FAISS.load_local(\n",
    "        \"chapter_summaries_vector_store\", embeddings, allow_dangerous_deserialization=True\n",
    "    )\n",
    "    book_quotes_vectorstore = FAISS.load_local(\n",
    "        \"book_quotes_vectorstore\", embeddings, allow_dangerous_deserialization=True\n",
    "    )\n",
    "else:\n",
    "    print(\"not\")\n",
    "    # If vector stores do not exist, encode and save them\n",
    "\n",
    "    # 1. Encode the book into a vector store of chunks\n",
    "    chunks_vector_store = encode_book(hp_pdf_path, chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "    # 2. Encode the chapter summaries into a vector store\n",
    "    chapter_summaries_vector_store = encode_chapter_summaries(chapter_summaries)\n",
    "\n",
    "    # 3. Encode the book quotes into a vector store\n",
    "    book_quotes_vectorstore = encode_quotes(book_quotes_list)\n",
    "\n",
    "    # 4. Save the vector stores to disk for future use\n",
    "    chunks_vector_store.save_local(\"chunks_vector_store\")\n",
    "    chapter_summaries_vector_store.save_local(\"chapter_summaries_vector_store\")\n",
    "    book_quotes_vectorstore.save_local(\"book_quotes_vectorstore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a4cf5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create Query Retrievers from Vector Stores ---\n",
    "\n",
    "# The following retrievers are used to fetch relevant documents from the vector stores\n",
    "# based on a query. The number of results returned can be controlled via the 'k' parameter.\n",
    "\n",
    "# Retriever for book chunks (returns the top 1 most relevant chunk)\n",
    "chunks_query_retriever = chunks_vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# Retriever for chapter summaries (returns the top 1 most relevant summary)\n",
    "chapter_summaries_query_retriever = chapter_summaries_vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# Retriever for book quotes (returns the top 10 most relevant quotes)\n",
    "book_quotes_query_retriever = book_quotes_vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "06dd66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import helper_functions\n",
    "importlib.reload(helper_functions)\n",
    "def retrieve_context_per_question(state):\n",
    "    \"\"\"\n",
    "    Retrieves relevant context for a given question by aggregating content from:\n",
    "    - Book chunks\n",
    "    - Chapter summaries\n",
    "    - Book quotes\n",
    "\n",
    "    Args:\n",
    "        state (dict): A dictionary containing the question to answer, with key \"question\".\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with keys:\n",
    "            - \"context\": Aggregated context string from all sources.\n",
    "            - \"question\": The original question.\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieve relevant book chunks\n",
    "    print(\"Retrieving relevant chunks...\")\n",
    "    docs = chunks_query_retriever.get_relevant_documents(question)\n",
    "    context = \" \".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Retrieve relevant chapter summaries\n",
    "    print(\"Retrieving relevant chapter summaries...\")\n",
    "    docs_summaries = chapter_summaries_query_retriever.get_relevant_documents(question)\n",
    "    context_summaries = \" \".join(\n",
    "        f\"{doc.page_content} (Chapter {doc.metadata['chapter']})\" for doc in docs_summaries\n",
    "    )\n",
    "\n",
    "    # Retrieve relevant book quotes\n",
    "    print(\"Retrieving relevant book quotes...\")\n",
    "    docs_book_quotes = book_quotes_query_retriever.get_relevant_documents(question)\n",
    "    book_quotes = \" \".join(doc.page_content for doc in docs_book_quotes)\n",
    "    # Aggregate all contexts and escape problematic characters\n",
    "    all_contexts = context + context_summaries + book_quotes\n",
    "    all_contexts =helper_functions.escape_quotes(all_contexts)\n",
    "\n",
    "    return {\"context\": all_contexts, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "098c34d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "importlib.reload(helper_functions)\n",
    "# --- Prompt template ---\n",
    "keep_only_relevant_content_prompt_template = \"\"\"\n",
    "You receive a query: {query} and retrieved documents: {retrieved_documents} from a vector store.\n",
    "You need to filter out all the non relevant information that doesn't supply important information regarding the {query}.\n",
    "Your goal is just to filter out the non relevant information.\n",
    "You can remove parts of sentences that are not relevant to the query or remove whole sentences that are not relevant to the query.\n",
    "DO NOT ADD ANY NEW INFORMATION THAT IS NOT IN THE RETRIEVED DOCUMENTS.\n",
    "Output the filtered relevant content.\n",
    "\"\"\"\n",
    "\n",
    "# --- Output schema ---\n",
    "class KeepRelevantContent(BaseModel):\n",
    "    relevant_content: str = Field(\n",
    "        description=\"The relevant content from the retrieved documents that is relevant to the query.\"\n",
    "    )\n",
    "\n",
    "# --- Prompt + chain ---\n",
    "keep_only_relevant_content_prompt = PromptTemplate(\n",
    "    template=keep_only_relevant_content_prompt_template,\n",
    "    input_variables=[\"query\", \"retrieved_documents\"],\n",
    ")\n",
    "\n",
    "keep_only_relevant_content_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    temperature=0,\n",
    "    convert_system_message_to_human=True\n",
    ")\n",
    "\n",
    "keep_only_relevant_content_chain = (\n",
    "    keep_only_relevant_content_prompt\n",
    "    | keep_only_relevant_content_llm.with_structured_output(KeepRelevantContent)\n",
    ")\n",
    "\n",
    "# --- Filtering function ---\n",
    "def keep_only_relevant_content(state):\n",
    "    \"\"\"\n",
    "    Filters and keeps only the relevant content from the retrieved documents that is relevant to the query.\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    input_data = {\n",
    "        \"query\": question,\n",
    "        \"retrieved_documents\": context\n",
    "    }\n",
    "\n",
    "    print(\"Keeping only the relevant content...\")\n",
    "    pprint(\"--------------------\")\n",
    "    output = keep_only_relevant_content_chain.invoke(input_data)\n",
    "    relevant_content = output.relevant_content\n",
    "    relevant_content = \"\".join(relevant_content)\n",
    "\n",
    "    # Escape quotes if needed (define escape_quotes function elsewhere if used)\n",
    "    relevant_content = helper_functions.escape_quotes(relevant_content)\n",
    "\n",
    "    return {\n",
    "        \"relevant_context\": relevant_content,\n",
    "        \"context\": context,\n",
    "        \"question\": question\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c64f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM-based Function to Rewrite a Question for Better Vectorstore Retrieval ---\n",
    "\n",
    "class RewriteQuestion(BaseModel):\n",
    "    \"\"\"\n",
    "    Output schema for the rewritten question.\n",
    "    \"\"\"\n",
    "    rewritten_question: str = Field(\n",
    "        description=\"The improved question optimized for vectorstore retrieval.\"\n",
    "    )\n",
    "    explanation: str = Field(\n",
    "        description=\"The explanation of the rewritten question.\"\n",
    "    )\n",
    "\n",
    "# Create a JSON parser for the output schema\n",
    "rewrite_question_string_parser = JsonOutputParser(pydantic_object=RewriteQuestion)\n",
    "\n",
    "# Initialize the LLM for rewriting questions\n",
    "rewrite_llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"llama3-70b-8192\",\n",
    "    groq_api_key=groq_api_key,\n",
    "    max_tokens=4000\n",
    ")\n",
    "\n",
    "# Define the prompt template for question rewriting\n",
    "rewrite_prompt_template = \"\"\"\n",
    "You are a question re-writer that converts an input question to a better version optimized for vectorstore retrieval.\n",
    "Analyze the input question {question}  like it maybe a trick way to answer a simple question and try to reason about the underlying semantic intent / meaning.\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "# Create the prompt object\n",
    "rewrite_prompt = PromptTemplate(\n",
    "    template=rewrite_prompt_template,\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": rewrite_question_string_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Combine prompt, LLM, and parser into a chain\n",
    "question_rewriter = rewrite_prompt | rewrite_llm | rewrite_question_string_parser\n",
    "\n",
    "def rewrite_question(state):\n",
    "    \"\"\"\n",
    "    Rewrites the given question using the LLM to optimize it for vectorstore retrieval.\n",
    "\n",
    "    Args:\n",
    "        state (dict): A dictionary containing the question to rewrite, with key \"question\".\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the rewritten question under the key \"question\".\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    print(\"Rewriting the question...\")\n",
    "    result = question_rewriter.invoke({\"question\": question})\n",
    "    new_question = result[\"rewritten_question\"]\n",
    "    return {\"question\": new_question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a028992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- LLM-based Function to Answer a Question from Context Using Chain-of-Thought Reasoning ---\n",
    "\n",
    "# Define the output schema for the answer\n",
    "class QuestionAnswerFromContext(BaseModel):\n",
    "    answer_based_on_content: str = Field(\n",
    "        description=\"Generates an answer to a query based on a given context.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Initialize the LLM for answering questions with chain-of-thought reasoning\n",
    "question_answer_from_context_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    temperature=0,\n",
    "    convert_system_message_to_human=True\n",
    ")\n",
    "# Define the prompt template with chain-of-thought examples and instructions\n",
    "question_answer_cot_prompt_template = \"\"\" \n",
    "Examples of Chain-of-Thought Reasoning\n",
    "\n",
    "Example 1\n",
    "\n",
    "Context: Mary is taller than Jane. Jane is shorter than Tom. Tom is the same height as David.\n",
    "Question: Who is the tallest person?\n",
    "Reasoning Chain:\n",
    "The context tells us Mary is taller than Jane\n",
    "It also says Jane is shorter than Tom\n",
    "And Tom is the same height as David\n",
    "So the order from tallest to shortest is: Mary, Tom/David, Jane\n",
    "Therefore, Mary must be the tallest person\n",
    "\n",
    "Example 2\n",
    "Context: Harry was reading a book about magic spells. One spell allowed the caster to turn a person into an animal for a short time. Another spell could levitate objects.\n",
    " A third spell created a bright light at the end of the caster's wand.\n",
    "Question: Based on the context, if Harry cast these spells, what could he do?\n",
    "Reasoning Chain:\n",
    "The context describes three different magic spells\n",
    "The first spell allows turning a person into an animal temporarily\n",
    "The second spell can levitate or float objects\n",
    "The third spell creates a bright light\n",
    "If Harry cast these spells, he could turn someone into an animal for a while, make objects float, and create a bright light source\n",
    "So based on the context, if Harry cast these spells he could transform people, levitate things, and illuminate an area\n",
    "Instructions.\n",
    "\n",
    "Example 3 \n",
    "Context: Harry Potter woke up on his birthday to find a present at the end of his bed. He excitedly opened it to reveal a Nimbus 2000 broomstick.\n",
    "Question: Why did Harry receive a broomstick for his birthday?\n",
    "Reasoning Chain:\n",
    "The context states that Harry Potter woke up on his birthday and received a present - a Nimbus 2000 broomstick.\n",
    "However, the context does not provide any information about why he received that specific present or who gave it to him.\n",
    "There are no details about Harry's interests, hobbies, or the person who gifted him the broomstick.\n",
    "Without any additional context about Harry's background or the gift-giver's motivations, there is no way to determine the reason he received a broomstick as a birthday present.\n",
    "\n",
    "For the question below, provide your answer by first showing your step-by-step reasoning process, breaking down the problem into a chain of thought before arriving at the final answer,\n",
    " just like in the previous examples.\n",
    "Context\n",
    "{context}\n",
    "Question\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt object\n",
    "question_answer_from_context_cot_prompt = PromptTemplate(\n",
    "    template=question_answer_cot_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"],\n",
    ")\n",
    "\n",
    "# Combine the prompt and LLM into a chain with structured output\n",
    "question_answer_from_context_cot_chain = (\n",
    "    question_answer_from_context_cot_prompt\n",
    "    | question_answer_from_context_llm.with_structured_output(QuestionAnswerFromContext)\n",
    ")\n",
    "\n",
    "def answer_question_from_context(state):\n",
    "    \"\"\"\n",
    "    Answers a question from a given context using chain-of-thought reasoning.\n",
    "\n",
    "    Args:\n",
    "        state (dict): A dictionary containing:\n",
    "            - \"question\": The query question.\n",
    "            - \"context\" or \"aggregated_context\": The context to answer the question from.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - \"answer\": The answer to the question from the context.\n",
    "            - \"context\": The context used.\n",
    "            - \"question\": The original question.\n",
    "    \"\"\"\n",
    "    # Use 'aggregated_context' if available, otherwise fall back to 'context'\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"aggregated_context\"] if \"aggregated_context\" in state else state[\"context\"]\n",
    "\n",
    "    input_data = {\n",
    "        \"question\": question,\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "    print(\"Answering the question from the retrieved context...\")\n",
    "\n",
    "    # Invoke the LLM chain to get the answer\n",
    "    output = question_answer_from_context_cot_chain.invoke(input_data)\n",
    "    answer = output.answer_based_on_content\n",
    "    print(f'answer before checking hallucination: {answer}')\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"context\": context,\n",
    "        \"question\": question\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cc6ba6",
   "metadata": {},
   "source": [
    "LLM based function to determine if retrieved content is relevant to the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c64d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM-based Function to Determine Relevance of Retrieved Content ---\n",
    "\n",
    "# Prompt template for checking if the retrieved context is relevant to the query\n",
    "is_relevant_content_prompt_template = \"\"\"\n",
    "You receive a query: {query} and a context: {context} retrieved from a vector store. \n",
    "You need to determine if the document is relevant to the query. \n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "# Output schema for the relevance check\n",
    "class Relevance(BaseModel):\n",
    "    is_relevant: bool = Field(description=\"Whether the document is relevant to the query.\")\n",
    "    explanation: str = Field(description=\"An explanation of why the document is relevant or not.\")\n",
    "\n",
    "# JSON parser for the output schema\n",
    "is_relevant_json_parser = JsonOutputParser(pydantic_object=Relevance)\n",
    "\n",
    "# Initialize the LLM for relevance checking\n",
    "is_relevant_llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"llama3-70b-8192\",\n",
    "    groq_api_key=groq_api_key,\n",
    "    max_tokens=4000\n",
    ")\n",
    "\n",
    "# Create the prompt object for the LLM\n",
    "is_relevant_content_prompt = PromptTemplate(\n",
    "    template=is_relevant_content_prompt_template,\n",
    "    input_variables=[\"query\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": is_relevant_json_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Combine prompt, LLM, and parser into a chain\n",
    "is_relevant_content_chain = is_relevant_content_prompt | is_relevant_llm | is_relevant_json_parser\n",
    "def is_relevant_content(state):\n",
    "    \"\"\"\n",
    "    Determines if the retrieved context is relevant to the query.\n",
    "\n",
    "    Args:\n",
    "        state (dict): A dictionary containing:\n",
    "            - \"question\": The query question.\n",
    "            - \"context\": The retrieved context to check for relevance.\n",
    "\n",
    "    Returns:\n",
    "        str: \"relevant\" if the context is relevant, \"not relevant\" otherwise.\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    input_data = {\n",
    "        \"query\": question,\n",
    "        \"context\": context\n",
    "    }\n",
    "\n",
    "    # Invoke the LLM chain to determine if the document is relevant\n",
    "    output = is_relevant_content_chain.invoke(input_data)\n",
    "    print(\"Determining if the document is relevant...\")\n",
    "    if output[\"is_relevant\"]:\n",
    "        print(\"The document is relevant.\")\n",
    "        return \"relevant\"\n",
    "    else:\n",
    "        print(\"The document is not relevant.\")\n",
    "        return \"not relevant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e111112",
   "metadata": {},
   "source": [
    "CHAIN to CHeck Grounded on FActs or Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "44199db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''--- LLM Chain to Check if an Answer is Grounded in the Provided Context ---'''\n",
    "\n",
    "# Define the output schema for the grounding check\n",
    "class IsGroundedOnFacts(BaseModel):\n",
    "    \"\"\"\n",
    "    Output schema for checking if the answer is grounded in the provided context.\n",
    "    \"\"\"\n",
    "    grounded_on_facts: bool = Field(description=\"Answer is grounded in the facts, 'yes' or 'no'\")\n",
    "\n",
    "# Initialize the LLM for fact-checking (using GPT-4o)\n",
    "is_grounded_on_facts_llm =ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"llama3-70b-8192\",\n",
    "    groq_api_key=groq_api_key,\n",
    "    max_tokens=4000\n",
    ")\n",
    "\n",
    "# Define the prompt template for fact-checking\n",
    "is_grounded_on_facts_prompt_template = \"\"\"\n",
    "You are a fact-checker that determines if the given answer {answer} is grounded in the given context {context}\n",
    "You don't mind if it doesn't make sense, as long as it is grounded in the context.\n",
    "Output a JSON with the field 'grounded_on_facts' set to True or False, and apart from the JSON format don't output any additional text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Create the prompt object\n",
    "is_grounded_on_facts_prompt = PromptTemplate(\n",
    "    template=is_grounded_on_facts_prompt_template,\n",
    "    input_variables=[\"context\", \"answer\"],\n",
    ")\n",
    "\n",
    "# Create the LLM chain for fact-checking\n",
    "is_grounded_on_facts_chain = (\n",
    "    is_grounded_on_facts_prompt\n",
    "    | is_grounded_on_facts_llm.with_structured_output(IsGroundedOnFacts)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7028820b",
   "metadata": {},
   "source": [
    "CAN the Question be Fully ANswered or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2862c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM Chain to Determine if a Question Can Be Fully Answered from Context ---\n",
    "\n",
    "# Define the prompt template for the LLM\n",
    "can_be_answered_prompt_template = \"\"\"\n",
    "You receive a query: {question} and a context: {context}. \n",
    "You need to determine if the question can be fully answered based on the context.\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "# Define the output schema for the LLM's response\n",
    "class QuestionAnswer(BaseModel):\n",
    "    can_be_answered: bool = Field(\n",
    "        description=\"binary result of whether the question can be fully answered or not\"\n",
    "    )\n",
    "    explanation: str = Field(\n",
    "        description=\"An explanation of why the question can be fully answered or not.\"\n",
    "    )\n",
    "\n",
    "# Create a JSON parser for the output schema\n",
    "can_be_answered_json_parser = JsonOutputParser(pydantic_object=QuestionAnswer)\n",
    "\n",
    "# Create the prompt object for the LLM\n",
    "answer_question_prompt = PromptTemplate(\n",
    "    template=can_be_answered_prompt_template,\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": can_be_answered_json_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Initialize the LLM (Groq Llama3) for this task\n",
    "can_be_answered_llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"llama3-70b-8192\",\n",
    "    groq_api_key=groq_api_key,\n",
    "    max_tokens=4000\n",
    ")\n",
    "\n",
    "# Compose the chain: prompt -> LLM -> output parser\n",
    "can_be_answered_chain = answer_question_prompt | can_be_answered_llm | can_be_answered_json_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b6440",
   "metadata": {},
   "source": [
    "Functions to call both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bd78e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Grades the generated answer to a question based on:\n",
    "    - Whether the answer is grounded in the provided context (fact-checking)\n",
    "    - Whether the question can be fully answered from the context\n",
    "\n",
    "    Args:\n",
    "        state (dict): A dictionary containing:\n",
    "            - \"context\": The context used to answer the question\n",
    "            - \"question\": The original question\n",
    "            - \"answer\": The generated answer\n",
    "\n",
    "    Returns:\n",
    "        str: One of \"hallucination\", \"useful\", or \"not_useful\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract relevant fields from state\n",
    "    context = state[\"context\"]\n",
    "    answer = state[\"answer\"]\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # 1. Check if the answer is grounded in the provided context (fact-checking)\n",
    "    print(\"Checking if the answer is grounded in the facts...\")\n",
    "    result = is_grounded_on_facts_chain.invoke({\"context\": context, \"answer\": answer})\n",
    "    grounded_on_facts = result.grounded_on_facts\n",
    "\n",
    "    if not grounded_on_facts:\n",
    "        # If not grounded, label as hallucination\n",
    "        print(\"The answer is hallucination.\")\n",
    "        return \"hallucination\"\n",
    "    else:\n",
    "        print(\"The answer is grounded in the facts.\")\n",
    "\n",
    "        # 2. Check if the question can be fully answered from the context\n",
    "        input_data = {\n",
    "            \"question\": question,\n",
    "            \"context\": context\n",
    "        }\n",
    "        print(\"Determining if the question is fully answered...\")\n",
    "        output = can_be_answered_chain.invoke(input_data)\n",
    "        can_be_answered = output[\"can_be_answered\"]\n",
    "\n",
    "        if can_be_answered:\n",
    "            print(\"The question can be fully answered.\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"The question cannot be fully answered.\")\n",
    "            return \"not_useful\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c61640",
   "metadata": {},
   "source": [
    "GRAPH till Now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "431696c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LangGraph Workflow Construction\n",
    "class QualitativeRetievalAnswerGraphState(TypedDict):\n",
    "    question: str\n",
    "    context: str\n",
    "    answer: str\n",
    "\n",
    "qualitative_retrieval_answer_workflow = StateGraph(QualitativeRetievalAnswerGraphState)\n",
    "\n",
    "qualitative_retrieval_answer_workflow.add_node(\"retrieve_context_per_question\", retrieve_context_per_question)\n",
    "qualitative_retrieval_answer_workflow.add_node(\"keep_only_relevant_content\", keep_only_relevant_content)\n",
    "qualitative_retrieval_answer_workflow.add_node(\"rewrite_question\", rewrite_question)\n",
    "qualitative_retrieval_answer_workflow.add_node(\"answer_question_from_context\", answer_question_from_context)\n",
    "\n",
    "qualitative_retrieval_answer_workflow.set_entry_point(\"retrieve_context_per_question\")\n",
    "qualitative_retrieval_answer_workflow.add_edge(\"retrieve_context_per_question\", \"keep_only_relevant_content\")\n",
    "qualitative_retrieval_answer_workflow.add_conditional_edges(\n",
    "    \"keep_only_relevant_content\",\n",
    "    is_relevant_content,\n",
    "    {\n",
    "        \"relevant\": \"answer_question_from_context\",\n",
    "        \"not relevant\": \"rewrite_question\"\n",
    "    },\n",
    ")\n",
    "qualitative_retrieval_answer_workflow.add_edge(\"rewrite_question\", \"retrieve_context_per_question\")\n",
    "qualitative_retrieval_answer_workflow.add_conditional_edges(\n",
    "    \"answer_question_from_context\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"hallucination\": \"answer_question_from_context\",\n",
    "        \"not_useful\": \"rewrite_question\",\n",
    "        \"useful\": END\n",
    "    },\n",
    ")\n",
    "\n",
    "qualitative_retrieval_answer_retrival_app = qualitative_retrieval_answer_workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55cdb1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "770f13bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(state):\n",
    "    return qualitative_retrieval_answer_retrival_app.invoke({\"question\": state[\"question\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "88336403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7874\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7874/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving relevant chunks...\n",
      "Retrieving relevant chapter summaries...\n",
      "Retrieving relevant book quotes...\n",
      "Keeping only the relevant content...\n",
      "'--------------------'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining if the document is relevant...\n",
      "The document is relevant.\n",
      "Answering the question from the retrieved context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer before checking hallucination: Reasoning Chain:\n",
      "The user is asking a question about a specific event in the provided text.\n",
      "The text includes a direct quote from Dumbledore to Professor McGonagall: “Would you care for a lemon drop?”.\n",
      "Professor McGonagall declines the offer.\n",
      "However, the provided context does not mention the location 'Privet Drive'.\n",
      "Therefore, based on the text, we can confirm the offer of the lemon drop but cannot confirm the location.\n",
      "\n",
      "Answer: Yes, based on the text, Professor Dumbledore offers Professor McGonagall a lemon drop. He says, “Would you care for a lemon drop?” However, the provided context does not specify that this exchange takes place on Privet Drive.\n",
      "Checking if the answer is grounded in the facts...\n",
      "The answer is grounded in the facts.\n",
      "Determining if the question is fully answered...\n",
      "The question can be fully answered.\n",
      "Checking if the answer is grounded in the facts...\n",
      "The answer is grounded in the facts.\n",
      "Determining if the question is fully answered...\n",
      "The question can be fully answered.\n",
      "Retrieving relevant chunks...\n",
      "Retrieving relevant chapter summaries...\n",
      "Retrieving relevant book quotes...\n",
      "Keeping only the relevant content...\n",
      "'--------------------'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining if the document is relevant...\n",
      "The document is relevant.\n",
      "Answering the question from the retrieved context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAGAS\\venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer before checking hallucination: Reasoning Chain:\n",
      "The user is asking about the smell of Mrs. Figg's house.\n",
      "The context explains that on Dudley's birthday, Harry was usually left with Mrs. Figg.\n",
      "The text describes Harry's feelings about being there and mentions a specific smell.\n",
      "It explicitly states, \"The whole house smelled of cabbage.\"\n",
      "\n",
      "Final Answer:\n",
      "Based on the text provided, Mrs. Figg's house smelled of cabbage.\n",
      "Checking if the answer is grounded in the facts...\n",
      "The answer is grounded in the facts.\n",
      "Determining if the question is fully answered...\n",
      "The question can be fully answered.\n",
      "Checking if the answer is grounded in the facts...\n",
      "The answer is grounded in the facts.\n",
      "Determining if the question is fully answered...\n",
      "The question can be fully answered.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# This is your RAG function using Gemini\n",
    "def rag_interface(question):\n",
    "    state = {\"question\": question}\n",
    "\n",
    "    # Step 1: Generate answer\n",
    "    output = generate_answer(state)\n",
    "\n",
    "    # Step 2: Grade the answer (grounded, useful, hallucination, etc.)\n",
    "    grade = grade_generation_v_documents_and_question(output)\n",
    "\n",
    "    # Step 3: Format final response\n",
    "    answer = output[\"answer\"]\n",
    "\n",
    "    if grade == \"useful\":\n",
    "        status = \"✅ Answer is grounded and useful.\"\n",
    "    elif grade == \"not_useful\":\n",
    "        status = \"⚠️ Answer is grounded but doesn't fully answer the question.\"\n",
    "    else:  # \"hallucination\"\n",
    "        status = \"🚫 Warning: The answer may not be grounded in the provided context.\"\n",
    "\n",
    "    return f\"{answer}\\n\\n---\\n{status}\"\n",
    "\n",
    "# Launch a Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=rag_interface,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Ask a question about the book...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"📘 Book Q&A with Gemini\",\n",
    "    description=\"Ask any question about the book. Answers are generated using Gemini and retrieved from book chunks, chapter summaries, and quotes.\"\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
